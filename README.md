# –†–∞–±–æ—Ç—É –≤—ã–ø–æ–ª–Ω–∏–ª–∏ —Å—Ç—É–¥–µ–Ω—Ç—ã 423 –≥—Ä—É–ø–ø—ã –ë–∞—Ä–∞–Ω–æ–≤–∞, –ë–µ–ª–∏–∫–æ–≤, –õ–µ—à—É–∫–æ–≤, –û—Ä–µ—Ö–æ–≤
---
# üß† SqueezeNet Classification ‚Äî Adam vs AMSGrad, Pretrained vs Scratch

## üìå –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ
[1. –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –∏ —Ü–µ–ª—å —Ä–∞–±–æ—Ç—ã](#title1)

[2. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞](#title2)

[3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è](#title3)

[4. –í—ã–≤–æ–¥—ã](#title4)

[5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏](#title4)

## üìå <a id="title1">1. –ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏ –∏ —Ü–µ–ª—å —Ä–∞–±–æ—Ç—ã</a>

–¶–µ–ª—å—é –¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞ —è–≤–ª—è–µ—Ç—Å—è:
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
- –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞—Ç–∞—Å–µ—Ç [Stanford Dogs Dataset](https://www.kaggle.com/datasets/jessicali9530/stanford-dogs-dataset/data)
- –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å —Å –∞—Ä–∑–∏—Ç–µ–∫—Ç—É—Ä–æ–π SqueezeNet
- –ü—Ä–æ–≤–µ—Å—Ç–∏ –¥–≤–∞ —Ç–∏–ø–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤:
  - –î–æ–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–º Adam –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–º AMSGrad
  - –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –Ω—É–ª—è –Ω–∞ Adam. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–µ–Ω–Ω–æ–π —Å –Ω—É–ª—è –∏ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏.
- –°–æ—Å—Ç–∞–≤–∏—Ç—å –æ—Ç—á–µ—Ç

---

## üìå <a id="title2">2. –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –±–∞–∑–∞</a>

### üîπ [**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ SqueezeNet**](https://medium.com/@avidrishik/squeezenets-architecture-compressed-neural-network-7741d24ca56f)

SqueezeNet ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è CNN-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ —É—Ä–æ–≤–Ω—è AlexNet –ø—Ä–∏ 50√ó –º–µ–Ω—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –Ω–∞ –ø—Ä–∏–Ω—Ü–∏–ø–µ ‚Äú—Å–∂–∞—Ç—å-—Ä–∞—Å—à–∏—Ä–∏—Ç—å‚Äù (squeeze-expand). Fire –º–æ–¥—É–ª–∏ —Å–Ω–∞—á–∞–ª–∞ —É–º–µ–Ω—å—à–∞—é—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤ —Å–≤–µ—Ä—Ç–∫–∞–º–∏ 1√ó1 (squeeze), –∞ –∑–∞—Ç–µ–º —Ä–∞—Å—à–∏—Ä—è—é—Ç –∏—Ö —á–µ—Ä–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ 1√ó1 –∏ 3√ó3 —Å–≤–µ—Ä—Ç–∫–∏ (expand). –†–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî –∫–æ–º–ø–∞–∫—Ç–Ω–∞—è –º–æ–¥–µ–ª—å —Å —Ö–æ—Ä–æ—à–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º.

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è,
- –Ω–∏–∑–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏,
- –ø—Ä–∏–µ–º–ª–µ–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –Ω–∏–∑–∫–∏—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è—Ö.

<p>
$$
\text{Squeeze layer: } s = \sigma(W_s * x + b_s)
$$
</p>

$x$ ‚Äî –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä

$W_s$ ‚Äî –≤–µ—Å–∞ 1x1 —Å–≤–µ—Ä—Ç–∫–∏

$b_s$ ‚Äî —Å–º–µ—â–µ–Ω–∏–µ

$\sigma$ ‚Äî —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (ReLU)

Expand —Å–ª–æ–π (1x1 –∏ 3x3 —Å–≤–µ—Ä—Ç–∫–∏):

<p>
$$
\text{Expand layer 1x1: } e_{1x1} = \sigma(W_{1x1} * s + b_{1x1})
$$
</p>

<p>
$$
\text{Expand layer 3x3: } e_{3x3} = \sigma(W_{3x3} * s + b_{3x3})
$$
</p>

---

### üîπ [**–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam**](https://www.geeksforgeeks.org/deep-learning/adam-optimizer/)
–û–¥–∏–Ω –∏–∑ —Å–∞–º—ã—Ö –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π.
Adam –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –∏–¥–µ–∏:
- Momentum (—É—Å–∫–æ—Ä—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ, —Å–≥–ª–∞–∂–∏–≤–∞–µ—Ç —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—é –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞)
- RMSProp (—Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç —à–∞–≥ –æ–±—É—á–µ–Ω–∏—è –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞)

–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –¥–≤–∞ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Å–≥–ª–∞–∂–µ–Ω–Ω—ã—Ö –º–æ–º–µ–Ω—Ç–∞:  
–ø–µ—Ä–≤—ã–π (m) ‚Äî –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π –≥—Ä–∞–¥–∏–µ–Ω—Ç (–ø–æ–º–æ–≥–∞–µ—Ç —Å–≥–ª–∞–¥–∏—Ç—å —à—É–º –≤ –æ–±—É—á–µ–Ω–∏–∏),  
–≤—Ç–æ—Ä–æ–π (v) ‚Äî –ù–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–π —Å—Ä–µ–¥–Ω–∏–π –∫–≤–∞–¥—Ä–∞—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ (—É–º–µ–Ω—å—à–∞–µ—Ç —à–∞–≥ –ø—Ä–∏ –±–æ–ª—å—à–∏—Ö –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞—Ö).  

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ Adam:
- –ë—ã—Å—Ç—Ä–æ —Å—Ö–æ–¥–∏—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–µ –∑–∞–¥–∞—á
- –£—Å—Ç–æ–π—á–∏–≤ –∫ —à—É–º–Ω—ã–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞–º
- –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤, –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π
- –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–≥–æ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
- –û–±–Ω–æ–≤–ª—è–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–¥–∞–ø—Ç–∏–≤–Ω–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–µ—Å–∞

<a id="Adam">–ì—Ä–∞–¥–∏–µ–Ω—Ç</a>:
<p>
$$
\text{Gradient: } g_t = \nabla_\theta L(\theta_t)
$$
</p>

–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –ø–µ—Ä–≤–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ (—Å–¥–≤–∏–≥):
<p>
$$
\text{First moment estimate: } m_t = \beta_1 m_{t-1} + (1 - \beta_1) g_t
$$
</p>

–°–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ç–æ—Ä–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ (–∫–≤–∞–¥—Ä–∞—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤):
<p>
$$
\text{Second moment estimate: } v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2
$$
</p>

–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è:
<p>
$$
\text{Bias-corrected estimates: } \hat{m}_t = \frac{m_t}{1 - \beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}
$$
</p>

–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
<p>
$$
\text{Parameter update: } \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$
</p>

---

### üîπ [**–û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä AMSGrad**](https://apxml.com/courses/optimization-techniques-ml/chapter-3-adaptive-learning-rate-algorithms/amsgrad-optimizer)

–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è Adam, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—â–∞—è –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ –≤—Ç–æ—Ä–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞:

- –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç "–≤—Å–ø–ª–µ—Å–∫–∏" learning rate,
- —É–ª—É—á—à–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å,
- –ª—É—á—à–µ –≤–µ–¥—ë—Ç —Å–µ–±—è –Ω–∞ –ø–ª–æ—Ö–æ –æ–±—É—Å–ª–æ–≤–ª–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö.

AMSGrad –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–µ–Ω, –µ—Å–ª–∏ –æ–±—É—á–µ–Ω–∏–µ "–∫–æ–ª–µ–±–ª–µ—Ç—Å—è" –∏–ª–∏ –ø–ª–æ—Ö–æ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è.

–ì—Ä–∞–¥–∏–µ–Ω—Ç, –ø–µ—Ä–≤—ã–π –º–æ–º–µ–Ω—Ç, –≤–æ—Ç–æ—Ä–æ–π –º–æ–º–µ–Ω—Ç –∫–∞–∫ –≤ [Adam](#Adam)

–ú–∞–∫—Å–∏–º—É–º –≤—Ç–æ—Ä–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞ (–∫–ª—é—á–µ–≤–æ–µ –æ—Ç–ª–∏—á–∏–µ):
<p>
$$
\text{Maintain maximum of second moment: } \hat{v}_t = \max(\hat{v}_{t-1}, v_t)
$$
</p>

–ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Å–º–µ—â–µ–Ω–∏—è –ø–µ—Ä–≤–æ–≥–æ –º–æ–º–µ–Ω—Ç–∞:
<p>
$$
\text{Bias-corrected first moment: } \hat{m}_t = \frac{m_t}{1 - \beta_1^t}
$$
</p>

–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤:
<p>
$$
\text{Parameter update: } \theta_{t+1} = \theta_t - \eta \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}
$$
</p>

---

## üìå <a id="title3">3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞–±–æ—Ç—ã –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è</a>

### üìä –ú–µ—Ç—Ä–∏–∫–∏, —Å–æ–±–∏—Ä–∞–µ–º—ã–µ —Å–∏—Å—Ç–µ–º–æ–π:

- **loss_val**
- **accuracy_val**
- **precision_val**
- **recall_val**
- **learning_rate**

–í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ `CSV/`


---

### üìà –ü—Ä–∏–º–µ—Ä—ã –≥—Ä–∞—Ñ–∏–∫–æ–≤

![–ì—Ä–∞—Ñ–∏–∫](PNG/fine_tuning_adam_VS_fine_tuning_amsgrad_combined.png)
![–ì—Ä–∞—Ñ–∏–∫](PNG/fine_tuning_adam_VS_scratch_adam_combined.png)


---

### üìã –¢–∞–±–ª–∏—Ü–∞ –∏—Ç–æ–≥–æ–≤—ã—Ö —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π

| –ú–æ–¥–µ–ª—å | –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è | –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä | Accuracy | Precision | Recall |
|--------|----------------|-------------|----------|-----------|--------|
| SqueezeNet | Pretrained | Adam | 0.63 | 0.63 | 0.63 |
| SqueezeNet | Pretrained | AMSGrad | 0.63 | 0.63 | 0.63 |
| SqueezeNet | Scratch | Adam | 0.22 | 0.21 | 0.21 |

---

## üìå <a id="title4">4. –í—ã–≤–æ–¥—ã</a>

–í —Ö–æ–¥–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –ø–æ–ª—É—á–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:

- –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç **–±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—É—é —Å—Ö–æ–¥–∏–º–æ—Å—Ç—å** –∏ **–±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é –∏—Ç–æ–≥–æ–≤—É—é —Ç–æ—á–Ω–æ—Å—Ç—å**.
- –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä **AMSGrad** –ø–æ–∫–∞–∑–∞–ª –Ω–µ–º–Ω–æ–≥–æ –ª—É—á—à—É—é —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å.
- –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è —Ç—Ä–µ–±—É–µ—Ç –Ω–∞–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —ç–ø–æ—Ö.
- –ü–æ—Å–ª–µ 15 —ç–ø–æ—Ö–∏ (–≤–≤–∏–¥—É —É–≤–µ–ª–∏—á–µ–Ω–∏—è loss) –Ω–∞–±–ª—é–¥–∞–µ—Ç—Å—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ.

---

## üìå <a id="title5">5. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏</a>

1. https://arenda-server.cloud/blog/populjarnye-arhitektury-glubokogo-obuchenija-resnet-inceptionv3-squeezenet/ 
2. https://www.geeksforgeeks.org/deep-learning/adam-optimizer/
3. https://apxml.com/courses/optimization-techniques-ml/chapter-3-adaptive-learning-rate-algorithms/amsgrad-optimizer 
4. https://www.kaggle.com/code/ad271828/squeezenet-pytorch-fruit-classification/notebook

